from components.initialize import initialize
from components.recup_data import retrieve_api_data, retrieve_memory, trouver_diplome
from components.addToMemory import add_to_memory
from components.loadModel import load_model

# Initialisation
embedding_model, collection, api_collection = initialize()
model, tokenizer = load_model()


def has_age(text):
    return any(a in text.lower() for a in ["ans", "j'ai", "Ã¢ge", "age", "an"])


def has_diplome(text):
    return any(d in text.lower() for d in ["bts", "bac", "cap", "licence", "master", "diplÃ´me", "niveau"])


def has_ville(text):
    return any(v in text.lower() for v in ["paris", "lille", "marseille", "roubaix", "lyon", "nantes", "ville"])


def generate_llm_response(prompt, model, tokenizer, embedding_model, collection, api_collection):
    """ GÃ©nÃ¨re une rÃ©ponse intelligente et fluide pour un assistant dâ€™orientation scolaire """

    # ðŸ” RÃ©cupÃ©ration mÃ©moire + donnÃ©es
    memory = retrieve_memory(prompt, embedding_model, collection)
    api_data = retrieve_api_data(prompt, embedding_model, api_collection)

    # ðŸ§  Contexte conversationnel
    context_elements = []
    if memory:
        context_elements.append("ðŸ§  Historique rÃ©cent :\n" + "\n\n".join([str(m) for m in memory[-1:]]))
    context = "\n".join(context_elements)
    texte_global = f"{prompt} {context}".lower()

    # ðŸ” VÃ©rifie si des infos manquent
    questions = []
    if not has_age(texte_global):
        questions.append("Quel est ton Ã¢ge ?")
    if not has_diplome(texte_global):
        questions.append("Quel est ton dernier diplÃ´me obtenu ?")
    if not has_ville(texte_global):
        questions.append("Dans quelle ville vis-tu ?")

    if questions:
        response = "Pour mieux tâ€™aider, jâ€™ai besoin de quelques infos :\n" + "\n".join(f"- {q}" for q in questions)
        add_to_memory(prompt, response, embedding_model, collection)
        return response

    # ðŸ§  Dictionnaire des diplÃ´mes
    diplomes_reference = {}
    for item in api_data:
        formation = item.get("Formation", "").lower()
        domaine = item.get("Domaine", "inconnu")
        niveau = item.get("Niveau de sortie", "inconnu")

    if any(x in formation for x in ["bts", "bac pro", "cap", "licence", "master","bac"]):
        diplomes_reference[formation] = {
            "niveau": niveau,
            "domaine": domaine
        }
        print("diplomes references ",diplomes_reference)

    # ðŸŽ¯ Matching du diplÃ´me
    match_diplome, infos_diplome = trouver_diplome(prompt, diplomes_reference)
    if match_diplome:
        context_elements.append(
            f"ðŸŽ“ DiplÃ´me identifiÃ© : {match_diplome} ({infos_diplome['niveau']}, domaine : {infos_diplome['domaine']})"
        )

        # ðŸ” Filtrage sÃ©mantique du domaine
        domaine_oriente = infos_diplome["domaine"]
        vector = embedding_model.encode(domaine_oriente).tolist()

        results = api_collection.query(
            query_embeddings=[vector],
            n_results=5,
            include=["documents", "metadatas"]
        )

        api_data = []
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            api_data.append({
                "Formation": meta.get("Formation", doc),
                "Ã‰tablissement": meta.get("Ã‰tablissement", "Inconnu"),
                "Ville": meta.get("Ville", "Inconnue"),
                "Domaine": meta.get("Domaine", "Inconnu")
            })

        # ðŸ“š Suggestions concrÃ¨tes
        if api_data:
            api_data_summary = api_data[:3]
            formatted_infos = "\n".join(
                [f"- {formation['Formation']} Ã  {formation['Ã‰tablissement']} ({formation['Ville']})"
                for formation in api_data_summary if isinstance(formation, dict)]
            )
            context_elements.append(f"ðŸ“š Formations suggÃ©rÃ©es :\n{formatted_infos}")

    # ðŸ” Reconstruit le contexte aprÃ¨s ajout
    context = "\n".join(context_elements)

    # ðŸ§  Prompt intelligent
    prompt_template = f"""
Tu es un conseiller d'orientation scolaire bienveillant, prÃ©cis, clair et proactif.

Contexte disponible :
{context}

Ta mission :
1. Si l'utilisateur semble perdu ou ne sait pas quoi faire, commence par lui poser les 3 questions suivantes :
   - Quel est ton Ã¢ge ?
   - Quel est ton dernier diplÃ´me obtenu ?
   - OÃ¹ vis-tu actuellement (ville ou rÃ©gion) ?

2. Une fois ces infos connues, propose-lui 2 Ã  3 formations adaptÃ©es.

3. Ne rÃ©pÃ¨te JAMAIS une question dÃ©jÃ  posÃ©e (analyse le contexte).

Utilisateur : {prompt}
Assistant :
""".strip()

    # âœ‚ï¸ DÃ©coupe si trop long
    prompt_tokens = tokenizer(prompt_template).input_ids
    MAX_TOKENS = 450
    if len(prompt_tokens) > MAX_TOKENS:
        prompt_tokens = prompt_tokens[-MAX_TOKENS:]
        prompt_template = tokenizer.decode(prompt_tokens, skip_special_tokens=True)

    # ðŸ§  Appel du modÃ¨le
    response = model(prompt_template, max_new_tokens=300, temperature=0.7, repetition_penalty=1.15,
                    stop=["Utilisateur :", "Assistant :"])
    add_to_memory(prompt, response, embedding_model, collection)
    return response
